\section{Research}

This section overviews the research I am undertaking as a part of this PhD. It is split into two subsections:
Current; covering the papers already submitted or under review, and Future; covering 
the research I have started working on or plan to work on in the next few months.

\subsection{Current}

\textbf{XAIStories}\\
In today's critical domains, the predominance of black-box machine learning models amplifies the demand for 
Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too 
intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present 
`what ifs' but leave users grappling with the 'why'. To bridge this gap, we introduce XAIstories. 
Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: 
SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF 
explanations to explain a decision. Our results are striking: over 90\% of the surveyed general audience 
finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories 
in communicating explanations to a general audience, with 92\% of data scientists indicating that it will contribute 
to the ease and confidence of nonspecialists in understanding AI predictions. Additionally, 83\% of data 
scientists indicate they are likely to use SHAPstories for this purpose. In image classification, 
CFstories are considered more or equally convincing as users own crafted stories by over 75\% of lay 
user participants. CFstories also bring a tenfold speed gain in creating a narrative, and improves accuracy by 
over 20\% compared to manually created narratives. The results thereby suggest that XAIstories may provide the 
missing link in truly explaining and understanding AI predictions.\\
\textbf{Aim: To enhance the understanding behind AI predictions for lay-users by providing natural language
        explanations.}\\
\textit{Currently revising for Decision Support Systems.}

\noindent
\textbf{CoF Tables}\\
The rise of deep learning in image classification has brought unprecedented accuracy but also highlighted a key 
issue: the use of `shortcuts' by models. Such shortcuts are easy-to-learn patterns from the training data that 
fail to generalise to new data. Examples include the use of a copyright watermark to recognise horses, snowy 
background to recognise huskies, or ink markings to detect malignant skin lesions. The explainable AI (XAI) 
community has suggested using instance-level explanations to detect shortcuts without external data, 
but this requires the examination of many explanations to confirm the presence of such shortcuts, making it a 
labour-intensive process. To address these challenges, we introduce Counterfactual Frequency (CoF) tables, 
a novel approach that aggregates instance-based explanations into global insights, and exposes shortcuts. 
The aggregation implies the need for some semantic concepts to be used in the explanations, which we solve by 
labelling the segments of an image. We demonstrate the utility of CoF tables across several datasets, revealing 
the shortcuts learned from them.\\
\textbf{Aim: To detect shortcuts in image classification models through the aggregation of local explanations.}\\
\textit{Currently under review at KDD, likely resubmitting to NeurIPS.}

\subsection{Future}


\begin{itemize}
    \item \textbf{PCherry}
    Given a set of counterfactuals, model and data they explain, can we get a probablilty that there is cherry picking in their selection.
    \item \textbf{Something to do with Optimisation (Explanation Combination, further exploration, improvement of CoF tables).}
    \item \textbf{Personalised Explanation Improvement}
    \item \textbf{Fairness of Image Classification}
    \item \textbf{Fairness of Counterfactual Explanations Alrgorithms}
\end{itemize}