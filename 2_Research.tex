\section{Research}

This section overviews the research I am undertaking as part of this PhD.
It is split into two subsections:
\begin{enumerate}
    \item \textbf{Current}: This covers the projects I am currently engaged in. Projects that have had papers 
    submitted are highlighted in \textbf{bold}, while those still being actively worked on are in \textit{italics}.
\item \textbf{Future}: This outlines the research I plan to undertake in the near future.
\end{enumerate}

\subsection*{Current}

\textbf{XAIStories}\\
In today's critical domains, the predominance of black-box machine learning models amplifies the demand for 
Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too 
intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present 
`what ifs' but leave users grappling with the 'why'. To bridge this gap, we introduce XAIstories. 
Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: 
SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF 
explanations to explain a decision. Our results are striking: over 90\% of the surveyed general audience 
finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories 
in communicating explanations to a general audience, with 92\% of data scientists indicating that it will contribute 
to the ease and confidence of nonspecialists in understanding AI predictions. Additionally, 83\% of data 
scientists indicate they are likely to use SHAPstories for this purpose. In image classification, 
CFstories are considered more or equally convincing as users own crafted stories by over 75\% of lay 
user participants. CFstories also bring a tenfold speed gain in creating a narrative, and improves accuracy by 
over 20\% compared to manually created narratives. The results thereby suggest that XAIstories may provide the 
missing link in truly explaining and understanding AI predictions.\\
\textbf{Aim: To enhance the understanding behind AI predictions for lay-users by providing natural language
        explanations.}\\
\textit{Currently revising for Decision Support Systems.} \\
For further details, see the \href{https://arxiv.org/abs/2309.17057}{Link to arXiv paper}. \\

\noindent
\textbf{CoF Tables}\\
The rise of deep learning in image classification has brought unprecedented accuracy but also highlighted a key 
issue: the use of `shortcuts' by models. Such shortcuts are easy-to-learn patterns from the training data that 
fail to generalise to new data. Examples include the use of a copyright watermark to recognise horses, snowy 
background to recognise huskies, or ink markings to detect malignant skin lesions. The explainable AI (XAI) 
community has suggested using instance-level explanations to detect shortcuts without external data, 
but this requires the examination of many explanations to confirm the presence of such shortcuts, making it a 
labour-intensive process. To address these challenges, we introduce Counterfactual Frequency (CoF) tables, 
a novel approach that aggregates instance-based explanations into global insights, and exposes shortcuts. 
The aggregation implies the need for some semantic concepts to be used in the explanations, which we solve by 
labelling the segments of an image. We demonstrate the utility of CoF tables across several datasets, revealing 
the shortcuts learned from them.\\
\textbf{Aim: To detect shortcuts in image classification models through the aggregation of local explanations.}\\
\textit{Currently under review at SIGKDD, likely resubmitting to NeurIPS.}

\

\subsection*{Future}

Titles in bold are currently being worked on, those in italics have yet to be started.

\begin{itemize}
    \item \textbf{PCherry}: Given a set of counterfactuals what is the probablilty that they include 
    cherry-picked explanations. Collaboration with two members of my research group.
    \item \textbf{AXA Collaboration}: Improving mail switching LLM through the analysis of counterfactual explanations.
    \item \textit{Fairness in Image Classification}: An investigation into the fairness of image classification 
    models using CoF tables.
    \item \textit{PCherry Fairness Spin-Off}: Examines the propensity of counterfactual explanations to 
    fairwash through optimisations aimed at enhancing plausibility and sparsity.
    \item \textit{CoF Tables 2.0}: Improving CoF tables through combinations of segments, 
    new segmentation and edit functions. Ablation studies to measure effectiveness of different components.
    \item Personalised Explanation Improvement.
\end{itemize}